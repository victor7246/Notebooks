{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "Named entity recognition is a technique to detect entities of interests from word tokens. in NER, each sentence in splitted into word tokens. Each word token is tagged with the corresponding entity. Possible entities could be - person's name, orgnaization name, phone number, address, currency etc. In clinical domain, we can have entities like - disease condition, test, procedure, medications etc.\n",
    "\n",
    "We use i2b2 dataset which contains discharge summaries of several patients. The dataset is tagged with <b>person</b>, <b>treatment</b>, <b>test</b>, <b>problem</b>, <b>pronoun</b>. Words that are not tagged with any of the entities are tagged with <b>O</b>.\n",
    "\n",
    "We use Bidirectional LSTM model for token classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Layer, Input, LSTM, Embedding, Dense, Conv1D, TimeDistributed, Dropout, Bidirectional, BatchNormalization, GlobalAveragePooling1D, SpatialDropout1D\n",
    "#from keras_contrib.layers import CRF\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "#import os, csv, math, codecs\n",
    "\n",
    "#import spacy\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Convert raw input data to tabular coNLL03 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_concept(concept_str):\n",
    "    \"\"\"\n",
    "    takes string like\n",
    "    'c=\"asymptomatic\" 16:2 16:2||t=\"problem\"'\n",
    "    and returns dictionary like\n",
    "    {'t': 'problem', 'start_line': 16, 'start_pos': 2, 'end_line': 16, 'end_pos': 2}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        position_bit, problem_bit = concept_str.split('||')\n",
    "        t = problem_bit[3:-1]\n",
    "        \n",
    "        start_and_end_span = next(re.finditer('\\s\\d+:\\d+\\s\\d+:\\d+', concept_str)).span()\n",
    "        c = concept_str[3:start_and_end_span[0]-1]\n",
    "        c = [y for y in c.split(' ') if y.strip() != '']\n",
    "        c = ' '.join(c)\n",
    "\n",
    "        start_and_end = concept_str[start_and_end_span[0]+1 : start_and_end_span[1]]\n",
    "        start, end = start_and_end.split(' ')\n",
    "        start_line, start_pos = [int(x) for x in start.split(':')]\n",
    "        end_line, end_pos = [int(x) for x in end.split(':')]\n",
    "        \n",
    "    except:\n",
    "        print(concept_str)\n",
    "        raise\n",
    "    \n",
    "    return {\n",
    "        't': t, 'start_line': start_line, 'start_pos': start_pos, 'end_line': end_line, 'end_pos': end_pos,\n",
    "        'c': c, \n",
    "    }\n",
    "\n",
    "def build_label_vocab(base_dirs):\n",
    "    seen, label_vocab, label_vocab_size = set(['O']), {'O': 'O'}, 0\n",
    "    \n",
    "    for base_dir in base_dirs:\n",
    "        concept_dir = os.path.join(base_dir, 'concepts')\n",
    "\n",
    "        assert os.path.isdir(concept_dir), \"Directory structure doesn't match!\"\n",
    "\n",
    "        ids = set([x.split('.')[0] for x in os.listdir(concept_dir) if x.endswith('.con')])\n",
    "\n",
    "        for i in ids:\n",
    "            with open(os.path.join(concept_dir, '%s.txt.con' % i)) as f:\n",
    "                concepts = [process_concept(x.strip()) for x in f.readlines()]\n",
    "            for c in concepts:\n",
    "                if c['t'] not in seen:\n",
    "                    label_vocab_size += 1\n",
    "                    label_vocab[c['t']] = c['t'] # label_vocab_size\n",
    "                    seen.update([c['t']])\n",
    "    return label_vocab, label_vocab_size\n",
    "\n",
    "def reformatter(base, label_vocab, txt_dir = None, concept_dir = None):\n",
    "    if txt_dir is None: txt_dir = os.path.join(base, 'docs')\n",
    "    if concept_dir is None: concept_dir = os.path.join(base, 'concepts')\n",
    "    \n",
    "    assert os.path.isdir(txt_dir) and os.path.isdir(concept_dir), \"Directory structure doesn't match!\"\n",
    "    \n",
    "    txt_ids = set([x.split('.')[0] for x in os.listdir(txt_dir) if x.endswith('.txt')])\n",
    "    concept_ids = set([x.split('.')[0] for x in os.listdir(concept_dir) if x.endswith('.con')])\n",
    "    \n",
    "    assert txt_ids == concept_ids, (\n",
    "        \"id set doesn't match: txt - concept = %s, concept - txt = %s\"\n",
    "        \"\" % (str(txt_ids - concept_ids), str(concept_ids - txt_ids))\n",
    "    )\n",
    "    \n",
    "    ids = txt_ids\n",
    "    \n",
    "    reprocessed_texts = {}\n",
    "    for i in ids:\n",
    "        with open(os.path.join(txt_dir, '%s.txt' % i), mode='r') as f:\n",
    "            lines = f.readlines()\n",
    "            txt = [[y for y in x.strip().split(' ') if y.strip() != ''] for x in lines]\n",
    "            line_starts_with_space = [x.startswith(' ') for x in lines]\n",
    "        with open(os.path.join(concept_dir, '%s.txt.con' % i), mode='r') as f:\n",
    "            concepts = [process_concept(x.strip()) for x in f.readlines()]\n",
    "            \n",
    "        labels = [['O' for _ in line] for line in txt]\n",
    "        for c in concepts:\n",
    "            if c['start_line'] == c['end_line']:\n",
    "                line = c['start_line']-1\n",
    "                p_modifier = -1 if line_starts_with_space[line] else 0\n",
    "                text = (' '.join(txt[line][c['start_pos']+p_modifier:c['end_pos']+1+p_modifier])).lower()\n",
    "                #assert text == c['c'], (\n",
    "                #    \"Text mismatch! %s vs. %s (id: %s, line: %d)\\nFull line: %s\"\n",
    "                #    \"\" % (c['c'], text, i, line, txt[line])\n",
    "                #)\n",
    "                \n",
    "            for line in range(c['start_line']-1, c['end_line']):\n",
    "                p_modifier = -1 if line_starts_with_space[line] else 0\n",
    "                start_pos = c['start_pos']+p_modifier if line == c['start_line']-1 else 0\n",
    "                end_pos   = c['end_pos']+1+p_modifier if line == c['end_line']-1 else len(txt[line])\n",
    "                \n",
    "                if line == c['end_line'] - 1: labels[line][end_pos-1] = label_vocab[c['t']]                \n",
    "                if line == c['start_line'] - 1: labels[line][start_pos] = label_vocab[c['t']]\n",
    "                for j in range(start_pos + 1, end_pos-1): labels[line][j] = label_vocab[c['t']]\n",
    "            \n",
    "        joined_words_and_labels = [zip(txt_line, label_line) for txt_line, label_line in zip(txt, labels)]\n",
    "\n",
    "        out_str = '\\n\\n'.join(\n",
    "            ['\\n'.join(['%s\\t%s' % p for p in joined_line]) for joined_line in joined_words_and_labels]\n",
    "        )\n",
    "        \n",
    "        reprocessed_texts[i] = out_str\n",
    "        \n",
    "    return reprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab, label_vocab_size = build_label_vocab([\n",
    "    './NERdata/raw/Beth_Train/'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 'O', 'person': 'person', 'treatment': 'treatment', 'test': 'test', 'problem': 'problem', 'pronoun': 'pronoun'}\n"
     ]
    }
   ],
   "source": [
    "print (label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprocessed_texts = reformatter('./NERdata/raw/Beth_Train/', label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admission\tO\n",
      "Date\tO\n",
      ":\tO\n",
      "\n",
      "2014-10-14\tO\n",
      "\n",
      "Discharge\tO\n",
      "Date\tO\n",
      ":\tO\n",
      "\n",
      "2014-10-17\tO\n",
      "\n",
      "Date\tO\n",
      "of\tO\n",
      "Birth\tO\n",
      ":\tO\n",
      "\n",
      "1959-12-09\tO\n",
      "\n",
      "Sex\tO\n",
      ":\tO\n",
      "\n",
      "M\tO\n",
      "\n",
      "Service\tO\n",
      ":\tO\n",
      "\n",
      "CCU\tO\n",
      "\n",
      "HISTORY\tO\n",
      "OF\tO\n",
      "PRESENT\tO\n",
      "ILLNESS\tO\n",
      ":\tO\n",
      "\n",
      "This\tpronoun\n",
      "is\tO\n",
      "a\tO\n",
      "55\tO\n",
      "-\tO\n",
      "year-old\tO\n",
      "Caucasian\tO\n",
      "speaking\tO\n",
      "male\tO\n",
      "who\tperson\n",
      "is\tO\n",
      "a\tO\n",
      "smoker\tO\n",
      "and\tO\n",
      "has\tO\n",
      "a\tO\n",
      "family\tperson\n",
      "history\tO\n",
      "of\tO\n",
      "coronary\tproblem\n",
      "artery\tproblem\n",
      "disease\tproblem\n",
      ",\tO\n",
      "as\tO\n",
      "well\tO\n",
      "as\tO\n",
      "a\tO\n",
      "personal\tO\n",
      "history\tO\n",
      "of\tO\n",
      "hypertension\tproblem\n",
      ",\tO\n",
      "who\tperson\n",
      "experienced\tO\n",
      "multiple\tO\n",
      "episodes\tO\n",
      "of\tO\n",
      "10/10\tproblem\n",
      "substernal\tproblem\n",
      "chest\tproblem\n",
      "pain\tproblem\n",
      "radiating\tO\n",
      "down\tO\n",
      "his\tperson\n",
      "left\tO\n",
      "arm\tO\n",
      "last\tO\n",
      "night\tO\n",
      "with\tO\n",
      "his\tperson\n",
      "daily\tO\n",
      "activities\tO\n",
      ".\tO\n",
      "\n",
      "Each\tproblem\n",
      "episode\tproblem\n",
      "lasted\tO\n",
      "approximately\tO\n",
      "15\tO\n",
      "minutes\tO\n",
      "in\tO\n",
      "duration\tO\n",
      "and\tO\n",
      "resolved\tO\n",
      "on\tO\n",
      "their\tpronoun\n",
      "own\tO\n",
      ".\tO\n",
      "\n",
      "This\tpronoun\n",
      "morning\tO\n",
      "while\tO\n",
      "landscaping\tO\n",
      "the\tperson\n",
      "patient\tperson\n",
      "had\tO\n",
      "unremitting\tproblem\n",
      "12-05\tproblem\n",
      "pain\tproblem\n",
      "with\tO\n",
      "shortness\tproblem\n",
      "of\tproblem\n",
      "breath\tproblem\n",
      "and\tO\n",
      "diaphoresis\tproblem\n",
      ".\tO\n",
      "\n",
      "He\tperson\n",
      "presented\tO\n",
      "to\tO\n",
      "Deaconess-Nashoba\tO\n",
      "Hospital\tO\n",
      "Hospital\tO\n",
      "and\tO\n",
      "the\ttest\n",
      "first\ttest\n",
      "EKG\ttest\n",
      "was\tO\n",
      "found\tO\n",
      "to\tO\n",
      "have\tO\n",
      "ST\tproblem\n",
      "elevations\tproblem\n",
      "in\tproblem\n",
      "V1-V3\tproblem\n",
      "of\tO\n",
      "1-2\tO\n",
      "mm\tO\n",
      "with\tO\n",
      "T\tproblem\n",
      "wave\tproblem\n",
      "inversions\tproblem\n",
      "in\tO\n",
      "V4-6\tO\n",
      ",\tO\n",
      "I\tO\n",
      "and\tO\n",
      "L\tO\n",
      ",\tO\n",
      "which\tpronoun\n",
      "progressed\tO\n",
      "to\tO\n",
      "3-0167\tO\n",
      "W.\tO\n",
      "Seventh\tO\n",
      "Ave.\tO\n",
      "elevations\tO\n",
      "within\tO\n",
      "20\tO\n",
      "minutes\tO\n",
      ".\tO\n",
      "\n",
      "Heparin\ttreatment\n",
      "and\tO\n",
      "nitroglycerin\ttreatment\n",
      "drip\ttreatment\n",
      "were\tO\n",
      "started\tO\n",
      "with\tO\n",
      "10\tO\n",
      "mg\tO\n",
      "of\tO\n",
      "IV\ttreatment\n",
      "Retavase\ttreatment\n",
      "half\tO\n",
      "dose\tO\n",
      ".\tO\n",
      "\n",
      "He\tperson\n",
      "was\tO\n",
      "transferred\tO\n",
      "to\tO\n",
      "the\tO\n",
      "North\tO\n",
      "Adams\tO\n",
      "Regional\tO\n",
      "Hospital\tO\n",
      "for\tO\n",
      "catheterization\ttest\n",
      ".\tO\n",
      "\n",
      "Once\tO\n",
      "at\tO\n",
      "Hahnemann\tO\n",
      "General\tO\n",
      "Hospital\tO\n",
      "he\tperson\n",
      "received\tO\n",
      "heparin\ttreatment\n",
      "and\tO\n",
      "Integrilin\ttreatment\n",
      ".\tO\n",
      "\n",
      "Coronary\ttest\n",
      "angiography\ttest\n",
      "revealed\tO\n",
      "left\tO\n",
      "main\tO\n",
      ",\tO\n",
      "left\tO\n",
      "circumflex\tO\n",
      "and\tO\n",
      "RCA\tO\n",
      "normal\tO\n",
      "and\tO\n",
      "left\tproblem\n",
      "LAD\tproblem\n",
      "with\tproblem\n",
      "99%\tproblem\n",
      "midstenosis\tproblem\n",
      ".\tO\n",
      "\n",
      "A\ttreatment\n",
      "Hepacoat\ttreatment\n",
      "stent\ttreatment\n",
      "was\tO\n",
      "placed\tO\n",
      "with\tO\n",
      "no\tO\n",
      "residuals\ttest\n",
      "but\tO\n",
      "good\tO\n",
      "flow\tO\n",
      "after\tO\n",
      "nitroglycerin\ttreatment\n",
      "and\tO\n",
      "diltiazem\ttreatment\n",
      ".\tO\n",
      "\n",
      "The\tproblem\n",
      "chest\tproblem\n",
      "pain\tproblem\n",
      "persisted\tO\n",
      "afterwards\tO\n",
      "and\tO\n",
      "a\ttest\n",
      "relook\ttest\n",
      "catheterization\ttest\n",
      "was\tO\n",
      "performed\tO\n",
      "that\tpronoun\n",
      "revealed\tO\n",
      "no\tO\n",
      "occlusions\tproblem\n",
      ".\tO\n",
      "\n",
      "The\ttest\n",
      "right\ttest\n",
      "heart\ttest\n",
      "catheterization\ttest\n",
      "revealed\tO\n",
      "a\ttest\n",
      "cardiac\ttest\n",
      "output\ttest\n",
      "index\ttest\n",
      "of\tO\n",
      "4.11\tO\n",
      "and\tO\n",
      "2.15\tO\n",
      "respectively\tO\n",
      ".\tO\n",
      "\n",
      "RA\ttest\n",
      "pressure\ttest\n",
      "was\tO\n",
      "13\tO\n",
      ",\tO\n",
      "RV\ttest\n",
      "pressure\ttest\n",
      "45/7\tO\n",
      ",\tO\n",
      "wedge\ttest\n",
      "27\tO\n",
      "and\tO\n",
      "PA\ttest\n",
      "pressures\ttest\n",
      "of\tO\n",
      "42/21\tO\n",
      ".\tO\n",
      "\n",
      "He\tperson\n",
      "was\tO\n",
      "transferred\tO\n",
      "to\tO\n",
      "the\tO\n",
      "CC\tO\n",
      "for\tO\n",
      "monitoring\ttest\n",
      "and\tO\n",
      "he\tperson\n",
      "denied\tO\n",
      "having\tO\n",
      "further\tproblem\n",
      "pain\tproblem\n",
      ",\tO\n",
      "shortness\tproblem\n",
      "of\tproblem\n",
      "breath\tproblem\n",
      ",\tO\n",
      "nausea\tproblem\n",
      "or\tO\n",
      "vomiting\tproblem\n",
      ".\tO\n",
      "\n",
      "At\tO\n",
      "baseline\tO\n",
      "the\tperson\n",
      "patient\tperson\n",
      "denies\tO\n",
      "any\tO\n",
      "previous\tproblem\n",
      "chest\tproblem\n",
      "pain\tproblem\n",
      ",\tO\n",
      "pressure\tproblem\n",
      ",\tO\n",
      "dyspnea\tproblem\n",
      "on\tO\n",
      "exertion\tO\n",
      ",\tO\n",
      "orthopnea\tproblem\n",
      ",\tO\n",
      "PND\tproblem\n",
      "or\tO\n",
      "palpitations\tproblem\n",
      ".\tO\n",
      "\n",
      "PAST\tO\n",
      "MEDICAL\tO\n",
      "HISTORY\tO\n",
      ":\tO\n",
      "\n",
      "Only\tO\n",
      "hypertension\tproblem\n",
      ".\tO\n",
      "\n",
      "MEDICATIONS\tO\n",
      ":\tO\n",
      "\n",
      "Antihypertensive\ttreatment\n",
      "medications\ttreatment\n",
      "that\tpronoun\n",
      "the\tperson\n",
      "patient\tperson\n",
      "could\tO\n",
      "not\tO\n",
      "identify\tO\n",
      ".\tO\n",
      "\n",
      "ALLERGIES\tO\n",
      ":\tO\n",
      "\n",
      "The\tperson\n",
      "patient\tperson\n",
      "denies\tO\n",
      "having\tO\n",
      "any\tproblem\n",
      "drug\tproblem\n",
      "allergies\tproblem\n",
      ".\tO\n",
      "\n",
      "FAMILY\tperson\n",
      "HISTORY\tO\n",
      ":\tO\n",
      "\n",
      "Significant\tO\n",
      "for\tO\n",
      "a\tperson\n",
      "brother\tperson\n",
      "who\tperson\n",
      "had\tO\n",
      "an\tproblem\n",
      "MI\tproblem\n",
      "at\tO\n",
      "age\tO\n",
      "61\tO\n",
      ".\tO\n",
      "\n",
      "SOCIAL\tO\n",
      "HISTORY\tO\n",
      ":\tO\n",
      "\n",
      "He\tperson\n",
      "has\tO\n",
      "two\tperson\n",
      "kids\tperson\n",
      ",\tO\n",
      "works\tO\n",
      "as\tO\n",
      "a\tO\n",
      "landscaper\tO\n",
      "and\tO\n",
      "as\tO\n",
      "a\tO\n",
      "school\tO\n",
      "custodian\tO\n",
      "in\tO\n",
      "Ware\tO\n",
      ".\tO\n",
      "\n",
      "He\tperson\n",
      "has\tO\n",
      "a\tO\n",
      "15\tO\n",
      "to\tO\n",
      "20\tO\n",
      "pack-year\tO\n",
      "history\tO\n",
      ".\tO\n",
      "\n",
      "Smokes\tO\n",
      "1\tO\n",
      "pack-per-day\tO\n",
      "for\tO\n",
      "15\tO\n",
      "years\tO\n",
      "but\tO\n",
      "had\tO\n",
      "quit\tO\n",
      "for\tO\n",
      "20\tO\n",
      "years\tO\n",
      "and\tO\n",
      "recently\tO\n",
      "restarted\tO\n",
      ".\tO\n",
      "\n",
      "He\tperson\n",
      "drinks\tO\n",
      "two\tO\n",
      "glasses\tO\n",
      "of\tO\n",
      "wine\tO\n",
      "per\tO\n",
      "day\tO\n",
      "and\tO\n",
      "denies\tO\n",
      "any\tO\n",
      "use\tO\n",
      "of\tO\n",
      "illicit\tO\n",
      "substances\tO\n",
      ".\tO\n",
      "\n",
      "PHYSICAL\tO\n",
      "EXAMINATION\tO\n",
      "ON\tO\n",
      "ADMISSION\tO\n",
      ":\tO\n",
      "\n",
      "The\tperson\n",
      "patient\tperson\n",
      "was\tO\n",
      "afebrile\tproblem\n",
      ".\tO\n",
      "\n",
      "Blood\ttest\n",
      "pressure\ttest\n",
      "156/67\tO\n",
      ",\tO\n",
      "pulse\ttest\n",
      "80\tO\n",
      "and\tO\n",
      "respiratory\ttest\n",
      "rate\ttest\n",
      "14\tO\n",
      ".\tO\n",
      "\n",
      "Appearance\tO\n",
      ":\tO\n",
      "\n",
      "He\tperson\n",
      "was\tO\n",
      "laying\tO\n",
      "flat\tO\n",
      ",\tO\n",
      "very\tproblem\n",
      "tense\tproblem\n",
      ".\tO\n",
      "\n",
      "Skin\tO\n",
      "was\tO\n",
      "strong\tO\n",
      "build\tO\n",
      "and\tO\n",
      "was\tO\n",
      "in\tO\n",
      "no\tO\n",
      "apparent\tproblem\n",
      "distress\tproblem\n",
      ".\tO\n",
      "\n",
      "HEENT\tO\n",
      ":\tO\n",
      "\n",
      "He\tperson\n",
      "was\tO\n",
      "anicteric\tproblem\n",
      "with\tO\n",
      "moist\tO\n",
      "mucous\tO\n",
      "membranes\tO\n",
      ".\tO\n",
      "\n",
      "No\tO\n",
      "JVD\tproblem\n",
      "or\tO\n",
      "increased\tproblem\n",
      "JVP\tproblem\n",
      "was\tO\n",
      "appreciated\tO\n",
      "at\tO\n",
      "0\tO\n",
      "degrees\tO\n",
      ".\tO\n",
      "\n",
      "The\tperson\n",
      "patient\tperson\n",
      "did\tO\n",
      "not\tO\n",
      "have\tO\n",
      "any\tproblem\n",
      "carotid\tproblem\n",
      "bruits\tproblem\n",
      ",\tO\n",
      "abdominal\tproblem\n",
      "bruits\tproblem\n",
      "or\tO\n",
      "femoral\tproblem\n",
      "bruits\tproblem\n",
      ".\tO\n",
      "\n",
      "His\ttest\n",
      "PMI\ttest\n",
      "was\tO\n",
      "two\tO\n",
      "finger\ttest\n",
      "breath\ttest\n",
      "width\ttest\n",
      "in\tO\n",
      "the\tO\n",
      "fifth\tO\n",
      "intercostal\tO\n",
      "space\tO\n",
      "lateral\tO\n",
      "to\tO\n",
      "the\tO\n",
      "midclavicular\tO\n",
      "line\tO\n",
      ".\tO\n",
      "\n",
      "There\tO\n",
      "were\tO\n",
      "no\tO\n",
      "lifts\tproblem\n",
      "or\tO\n",
      "heaves\tproblem\n",
      "on\tO\n",
      "examination\ttest\n",
      ".\tO\n",
      "\n",
      "His\tperson\n",
      "heart\tO\n",
      "was\tO\n",
      "regular\tO\n",
      "with\tO\n",
      "no\tO\n",
      "murmurs\tproblem\n",
      ",\tO\n",
      "rubs\tproblem\n",
      "or\tO\n",
      "gallops\tproblem\n",
      "and\tO\n",
      "his\ttest\n",
      "radial\ttest\n",
      "and\ttest\n",
      "DP\ttest\n",
      "pulses\ttest\n",
      "were\tO\n",
      "both\tO\n",
      "2+\tO\n",
      "bilaterally\tO\n",
      "and\tO\n",
      "equal\tO\n",
      ".\tO\n",
      "\n",
      "His\tperson\n",
      "lungs\tO\n",
      "were\tO\n",
      "clear\tO\n",
      "to\tO\n",
      "auscultation\ttest\n",
      ".\tO\n",
      "\n",
      "His\tperson\n",
      "abdomen\tO\n",
      "was\tO\n",
      "scaphoid\tproblem\n",
      ",\tO\n",
      "nondistended\tproblem\n",
      ",\tO\n",
      "nontender\tproblem\n",
      "with\tO\n",
      "positive\tO\n",
      "bowel\tO\n",
      "sounds\tO\n",
      ".\tO\n",
      "\n",
      "His\tperson\n",
      "extremities\tO\n",
      "were\tO\n",
      "warm\tO\n",
      ",\tO\n",
      "pink\tO\n",
      "with\tO\n",
      "no\tO\n",
      "edema\tproblem\n",
      ".\tO\n",
      "\n",
      "The\tO\n",
      "right\tO\n",
      "calf\tO\n",
      "site\tO\n",
      "had\tO\n",
      "minimal\tproblem\n",
      "ooze\tproblem\n",
      "and\tO\n",
      "no\tO\n",
      "hematomas\tproblem\n",
      "or\tO\n",
      "bruits\tproblem\n",
      ".\tO\n",
      "\n",
      "ELECTROCARDIOGRAM\tO\n",
      ":\tO\n",
      "\n",
      "Significant\tO\n",
      "for\tO\n",
      "what\tO\n",
      "was\tO\n",
      "stated\tO\n",
      "in\tO\n",
      "the\tO\n",
      "HPI\tO\n",
      ".\tO\n",
      "\n",
      "OVERALL\tO\n",
      "IMPRESSION\tO\n",
      ":\tO\n",
      "\n",
      "This\tpronoun\n",
      "is\tO\n",
      "a\tO\n",
      "54\tO\n",
      "-\tO\n",
      "year-old\tO\n",
      "man\tO\n",
      "with\tO\n",
      "significant\tproblem\n",
      "coronary\tproblem\n",
      "artery\tproblem\n",
      "disease\tproblem\n",
      "risk\tproblem\n",
      "factors\tproblem\n",
      ",\tO\n",
      "positive\tO\n",
      "family\tperson\n",
      "history\tO\n",
      ",\tO\n",
      "smoking\tO\n",
      "history\tO\n",
      ",\tO\n",
      "hypertension\tproblem\n",
      "and\tO\n",
      "gender\tO\n",
      "who\tperson\n",
      "presented\tO\n",
      "with\tO\n",
      "an\tproblem\n",
      "ST\tproblem\n",
      "elevation\tproblem\n",
      "MI\tproblem\n",
      "status\tO\n",
      "post\tO\n",
      "LAD\ttreatment\n",
      "stent\ttreatment\n",
      ",\tO\n",
      "complicated\tO\n",
      "by\tO\n",
      "no\tO\n",
      "residuals\tproblem\n",
      "requiring\tO\n",
      "nitroglycerin\ttreatment\n",
      "and\tO\n",
      "diltiazem\ttreatment\n",
      "for\tO\n",
      "good\tO\n",
      "flow\tO\n",
      ".\tO\n",
      "\n",
      "The\tperson\n",
      "patient\tperson\n",
      "was\tO\n",
      "pain\tproblem\n",
      "free\tO\n",
      "and\tO\n",
      "doing\tO\n",
      "well\tO\n",
      ".\tO\n",
      "\n",
      "HOSPITAL\tO\n",
      "COURSE\tO\n",
      ":\tO\n",
      "\n",
      "For\tO\n",
      "his\tproblem\n",
      "coronary\tproblem\n",
      "artery\tproblem\n",
      "disease\tproblem\n",
      ",\tO\n",
      "aspirin\ttreatment\n",
      "was\tO\n",
      "started\tO\n",
      "at\tO\n",
      "325\tO\n",
      ".\tO\n",
      "\n",
      "Plavix\ttreatment\n",
      "was\tO\n",
      "started\tO\n",
      "at\tO\n",
      "75\tO\n",
      "mg\tO\n",
      "a\tO\n",
      "day\tO\n",
      ",\tO\n",
      "Lipitor\ttreatment\n",
      "20\tO\n",
      "mg\tO\n",
      "per\tO\n",
      "day\tO\n",
      ",\tO\n",
      "metoprolol\ttreatment\n",
      "25\tO\n",
      "mg\tO\n",
      "twice\tO\n",
      "a\tO\n",
      "day\tO\n",
      "to\tO\n",
      "be\tO\n",
      "titrated\tO\n",
      "up\tO\n",
      "as\tO\n",
      "tolerated\tO\n",
      "and\tO\n",
      "on\tO\n",
      "day\tO\n",
      "two\tO\n",
      "of\tO\n",
      "admission\tO\n",
      "an\ttreatment\n",
      "ACE\ttreatment\n",
      "inhibitor\ttreatment\n",
      "was\tO\n",
      "started\tO\n",
      ".\tO\n",
      "\n",
      "Captopril\ttreatment\n",
      "6.2\tO\n",
      "3\tO\n",
      "x\tO\n",
      "a\tO\n",
      "day\tO\n",
      "and\tO\n",
      "titrated\tO\n",
      "up\tO\n",
      ".\tO\n",
      "\n",
      "A\ttest\n",
      "lipid\ttest\n",
      "panel\ttest\n",
      "was\tO\n",
      "checked\tO\n",
      "and\tO\n",
      "found\tO\n",
      "to\tO\n",
      "be\tO\n",
      "within\tO\n",
      "normal\tO\n",
      "limits\tO\n",
      ",\tO\n",
      "although\tO\n",
      "it\tpronoun\n",
      "was\tO\n",
      "noted\tO\n",
      "this\tpronoun\n",
      "was\tO\n",
      "in\tO\n",
      "the\tO\n",
      "post\tO\n",
      "MI\tO\n",
      "setting\tO\n",
      "and\tO\n",
      "the\ttest\n",
      "lipid\ttest\n",
      "panel\ttest\n",
      "could\tO\n",
      "be\tO\n",
      "falsely\tO\n",
      "low\tO\n",
      ".\tO\n",
      "\n",
      "Serial\ttest\n",
      "CK\ttest\n",
      "s\ttest\n",
      "were\tO\n",
      "followed\tO\n",
      "with\tO\n",
      "a\ttest\n",
      "CK\ttest\n",
      "peak\ttest\n",
      "of\tO\n",
      "433\tO\n",
      ".\tO\n",
      "\n",
      "Hypertension\tproblem\n",
      "was\tO\n",
      "managed\tO\n",
      "with\tO\n",
      "beta\ttreatment\n",
      "blocker\ttreatment\n",
      "and\tO\n",
      "ACE\ttreatment\n",
      "inhibitor\ttreatment\n",
      "and\tO\n",
      "Integrilin\ttreatment\n",
      "was\tO\n",
      "continued\tO\n",
      "post\tO\n",
      "MI\tproblem\n",
      "for\tO\n",
      "18\tO\n",
      "hours\tO\n",
      ".\tO\n",
      "\n",
      "Rhythm\tO\n",
      "was\tO\n",
      "maintained\tO\n",
      "at\tO\n",
      "normal\tO\n",
      "sinus\tO\n",
      ",\tO\n",
      "monitored\tO\n",
      "on\tO\n",
      "telemetry\ttest\n",
      "without\tO\n",
      "ectopy\tproblem\n",
      ".\tO\n",
      "\n",
      "EKG\ttest\n",
      "s\ttest\n",
      "were\tO\n",
      "followed\tO\n",
      "for\tO\n",
      "normalization\tO\n",
      ".\tO\n",
      "\n",
      "The\tperson\n",
      "patient\tperson\n",
      "had\tO\n",
      "an\ttest\n",
      "echocardiogram\ttest\n",
      "on\tO\n",
      "day\tO\n",
      "two\tO\n",
      "of\tO\n",
      "admission\tO\n",
      ",\tO\n",
      "which\tpronoun\n",
      "revealed\tO\n",
      "a\tproblem\n",
      "mildly\tproblem\n",
      "dilated\tproblem\n",
      "left\tproblem\n",
      "atrium\tproblem\n",
      ",\tO\n",
      "mild\tproblem\n",
      "symmetric\tproblem\n",
      "LVH\tproblem\n",
      ",\tO\n",
      "normal\tO\n",
      "LV\tO\n",
      "cavity\tO\n",
      "size\tO\n",
      ",\tO\n",
      "mild\tproblem\n",
      "region\tproblem\n",
      "LV\tproblem\n",
      "systolic\tproblem\n",
      "dysfunction\tproblem\n",
      ",\tO\n",
      "arresting\tproblem\n",
      "regional\tproblem\n",
      "wall\tproblem\n",
      "motion\tproblem\n",
      "abnormality\tproblem\n",
      "including\tO\n",
      "focal\tproblem\n",
      "apical\tproblem\n",
      "hypokinesis\tproblem\n",
      ",\tO\n",
      "a\tO\n",
      "normal\tO\n",
      "right\tO\n",
      "ventricular\tO\n",
      "chamber\tO\n",
      "size\tO\n",
      "and\tO\n",
      "free\tO\n",
      "wall\tO\n",
      "motion\tO\n",
      ",\tO\n",
      "a\tproblem\n",
      "moderately\tproblem\n",
      "dilated\tproblem\n",
      "aortic\tproblem\n",
      "root\tproblem\n",
      ",\tO\n",
      "a\tproblem\n",
      "mildly\tproblem\n",
      "dilated\tproblem\n",
      "ascending\tproblem\n",
      "aorta\tproblem\n",
      ",\tO\n",
      "normal\tO\n",
      "aortic\tO\n",
      "valve\tO\n",
      "leaflet\tO\n",
      ",\tO\n",
      "normal\tO\n",
      "mitral\tO\n",
      "valve\tO\n",
      "leaflet\tO\n",
      "and\tO\n",
      "no\tO\n",
      "pericardial\tproblem\n",
      "effusions\tproblem\n",
      ".\tO\n",
      "\n",
      "The\tperson\n",
      "patient\tperson\n",
      "was\tO\n",
      "normal\tO\n",
      "volume\tO\n",
      "and\tO\n",
      "displayed\tO\n",
      "no\tO\n",
      "evidence\tO\n",
      "CHF\tproblem\n",
      "signs\tproblem\n",
      "or\tproblem\n",
      "symptoms\tproblem\n",
      ".\tO\n",
      "\n",
      "The\tproblem\n",
      "risk\tproblem\n",
      "factors\tproblem\n",
      "were\tO\n",
      "addressed\tO\n",
      ".\tO\n",
      "\n",
      "The\tperson\n",
      "patient\tperson\n",
      "was\tO\n",
      "told\tO\n",
      "repeatedly\tO\n",
      "that\tpronoun\n",
      "he\tperson\n",
      "needed\tO\n",
      "to\tO\n",
      "stop\tO\n",
      "smoking\tO\n",
      "and\tO\n",
      "was\tO\n",
      "given\tO\n",
      "a\ttreatment\n",
      "nicotine\ttreatment\n",
      "patch\ttreatment\n",
      "and\tO\n",
      "his\tperson\n",
      "primary\tperson\n",
      "care\tperson\n",
      "provider\tperson\n",
      "was\tO\n",
      "called\tO\n",
      "to\tO\n",
      "discuss\tO\n",
      "outpatient\tO\n",
      "plans\tO\n",
      "to\tO\n",
      "help\tO\n",
      "the\tperson\n",
      "patient\tperson\n",
      "stop\tO\n",
      "smoking\tO\n",
      ".\tO\n",
      "\n",
      "The\tperson\n",
      "patient\tperson\n",
      "was\tO\n",
      "advised\tO\n",
      "to\tO\n",
      "have\tO\n",
      "a\ttreatment\n",
      "low\ttreatment\n",
      "fat\ttreatment\n",
      "/\ttreatment\n",
      "low\ttreatment\n",
      "cholesterol\ttreatment\n",
      "diet\ttreatment\n",
      ",\tO\n",
      "was\tO\n",
      "cleared\tO\n",
      "by\tO\n",
      "PT\tO\n",
      "and\tO\n",
      "advised\tO\n",
      "to\tO\n",
      "do\tO\n",
      "cardiac\ttreatment\n",
      "rehabilitation\ttreatment\n",
      ".\tO\n",
      "\n",
      "He\tperson\n",
      "was\tO\n",
      "advised\tO\n",
      "the\tO\n",
      "importance\tO\n",
      "of\tO\n",
      "each\tO\n",
      "of\tO\n",
      "his\tperson\n",
      "and\tO\n",
      "compliance\tO\n",
      "with\tO\n",
      "each\tO\n",
      "of\tO\n",
      "these\tpronoun\n",
      "medications\ttreatment\n",
      "and\tO\n",
      "was\tO\n",
      "told\tO\n",
      "he\tperson\n",
      "needed\tO\n",
      "close\tO\n",
      "followup\tO\n",
      "with\tO\n",
      "a\tperson\n",
      "cardiologist\tperson\n",
      ".\tO\n",
      "\n",
      "For\tO\n",
      "renal\tO\n",
      ",\tO\n",
      "his\ttest\n",
      "creatinine\ttest\n",
      "was\tO\n",
      "monitored\tO\n",
      "and\tO\n",
      "the\ttest\n",
      "dye\ttest\n",
      "load\ttest\n",
      "and\tO\n",
      "the\ttest\n",
      "catheterization\ttest\n",
      ".\tO\n",
      "\n",
      "It\tpronoun\n",
      "was\tO\n",
      "normal\tO\n",
      "and\tO\n",
      "did\tO\n",
      "not\tO\n",
      "increase\tO\n",
      ".\tO\n",
      "\n",
      "Gastrointestinal\tO\n",
      ":\tO\n",
      "\n",
      "The\tperson\n",
      "patient\tperson\n",
      "was\tO\n",
      "given\tO\n",
      "a\ttreatment\n",
      "low\ttreatment\n",
      "fat\ttreatment\n",
      "/\ttreatment\n",
      "low\ttreatment\n",
      "cholesterol\ttreatment\n",
      "diet\ttreatment\n",
      ".\tO\n",
      "\n",
      "His\ttest\n",
      "electrolytes\ttest\n",
      "were\tO\n",
      "monitored\tO\n",
      "and\tO\n",
      "repleted\tO\n",
      "carefully\tO\n",
      ".\tO\n",
      "\n",
      "He\tperson\n",
      "had\tO\n",
      "normal\tO\n",
      "bowel\tO\n",
      "movements\tO\n",
      "and\tO\n",
      "had\tO\n",
      "GI\ttreatment\n",
      "prophylaxis\ttreatment\n",
      "throughout\tO\n",
      "his\tperson\n",
      "stay\tO\n",
      ".\tO\n",
      "\n",
      "For\tO\n",
      "hematology\tO\n",
      ",\tO\n",
      "the\tperson\n",
      "patient\tperson\n",
      "'s\ttest\n",
      "hematocrit\ttest\n",
      "and\tO\n",
      "platelets\ttest\n",
      "were\tO\n",
      "monitored\tO\n",
      "after\tO\n",
      "the\ttest\n",
      "catheterization\ttest\n",
      ".\tO\n",
      "\n",
      "They\tpronoun\n",
      "were\tO\n",
      "normal\tO\n",
      "and\tO\n",
      "did\tO\n",
      "not\tO\n",
      "change\tO\n",
      "and\tO\n",
      "the\tperson\n",
      "patient\tperson\n",
      "received\tO\n",
      "pneumatic\ttreatment\n",
      "boots\ttreatment\n",
      "for\tO\n",
      "DVT\ttreatment\n",
      "prophylaxis\ttreatment\n",
      ".\tO\n",
      "\n",
      "On\tO\n",
      "day\tO\n",
      "two\tO\n",
      "of\tO\n",
      "admission\tO\n",
      ",\tO\n",
      "the\tperson\n",
      "patient\tperson\n",
      "ambulated\tO\n",
      "with\tO\n",
      "PT\ttreatment\n",
      "and\tO\n",
      "was\tO\n",
      "moved\tO\n",
      "to\tO\n",
      "the\tO\n",
      "regular\tO\n",
      "floor\tO\n",
      ".\tO\n",
      "\n",
      "On\tO\n",
      "day\tO\n",
      "three\tO\n",
      "of\tO\n",
      "admission\tO\n",
      ",\tO\n",
      "the\tperson\n",
      "patient\tperson\n",
      "felt\tO\n",
      "that\tpronoun\n",
      "his\tperson\n",
      "strength\tO\n",
      "was\tO\n",
      "back\tO\n",
      "to\tO\n",
      "baseline\tO\n",
      ".\tO\n",
      "\n",
      "He\tperson\n",
      "denied\tO\n",
      "ever\tO\n",
      "having\tO\n",
      "any\tproblem\n",
      "chest\tproblem\n",
      "pain\tproblem\n",
      ",\tO\n",
      "chest\tproblem\n",
      "pressure\tproblem\n",
      ",\tO\n",
      "shortness\tproblem\n",
      "of\tproblem\n",
      "breath\tproblem\n",
      ",\tO\n",
      "dyspnea\tproblem\n",
      "on\tO\n",
      "exertion\tO\n",
      "and\tO\n",
      "he\tperson\n",
      "was\tO\n",
      "discharged\tO\n",
      "to\tO\n",
      "home\tO\n",
      ".\tO\n",
      "\n",
      "He\tperson\n",
      "was\tO\n",
      "told\tO\n",
      "to\tO\n",
      "return\tO\n",
      "to\tO\n",
      "the\tO\n",
      "emergency\tO\n",
      "department\tO\n",
      "if\tO\n",
      "he\tperson\n",
      "had\tO\n",
      "any\tproblem\n",
      "chest\tproblem\n",
      "pain\tproblem\n",
      ",\tO\n",
      "pressure\tproblem\n",
      ",\tO\n",
      "difficulty\tproblem\n",
      "breathing\tproblem\n",
      ",\tO\n",
      "nausea\tproblem\n",
      ",\tO\n",
      "light-headiness\tproblem\n",
      "or\tO\n",
      "dizziness\tproblem\n",
      ".\tO\n",
      "\n",
      "He\tperson\n",
      "was\tO\n",
      "advised\tO\n",
      "to\tO\n",
      "take\tO\n",
      "all\tO\n",
      "of\tO\n",
      "his\ttreatment\n",
      "medications\ttreatment\n",
      ".\tO\n",
      "\n",
      "An\tO\n",
      "appointment\tO\n",
      "was\tO\n",
      "established\tO\n",
      "for\tO\n",
      "him\tperson\n",
      "to\tO\n",
      "see\tO\n",
      "his\tperson\n",
      "primary\tperson\n",
      "care\tperson\n",
      "doctor\tperson\n",
      ",\tO\n",
      "Dr.\tperson\n",
      "Stella\tperson\n",
      "Booth\tperson\n",
      "on\tO\n",
      "Maynard\tO\n",
      ",\tO\n",
      "2014-10-20\tO\n",
      "and\tO\n",
      "the\tperson\n",
      "patient\tperson\n",
      "was\tO\n",
      "referred\tO\n",
      "to\tO\n",
      "Dr.\tperson\n",
      "Granville\tperson\n",
      "Hamers\tperson\n",
      ",\tO\n",
      "at\tO\n",
      "Ware\tO\n",
      "in\tO\n",
      "cardiology\tO\n",
      "to\tO\n",
      "be\tO\n",
      "seen\tO\n",
      "in\tO\n",
      "two\tO\n",
      "weeks\tO\n",
      ".\tO\n",
      "\n",
      "Dr\tperson\n",
      "Marcus\tperson\n",
      "assistant\tperson\n",
      "was\tO\n",
      "called\tO\n",
      "and\tO\n",
      "she\tperson\n",
      "stated\tO\n",
      "that\tpronoun\n",
      "she\tperson\n",
      "would\tO\n",
      "call\tO\n",
      "the\tperson\n",
      "patient\tperson\n",
      "with\tO\n",
      "an\tO\n",
      "appointment\tO\n",
      ".\tO\n",
      "\n",
      "FINAL\tO\n",
      "DIAGNOSES\tO\n",
      ":\tO\n",
      "\n",
      "1.\tO\n",
      "ST\tproblem\n",
      "elevation\tproblem\n",
      "myocardial\tproblem\n",
      "infarction\tproblem\n",
      ".\tO\n",
      "\n",
      "2.\tO\n",
      "Hypertension\tproblem\n",
      ".\tO\n",
      "\n",
      "MAJOR\tO\n",
      "SURGICAL\tO\n",
      "AND\tO\n",
      "INVASIVE\tO\n",
      "PROCEDURES\tO\n",
      ":\tO\n",
      "\n",
      "Cardiac\ttest\n",
      "catheterization\ttest\n",
      "and\tO\n",
      "stent\ttreatment\n",
      "placement\ttreatment\n",
      "in\ttreatment\n",
      "the\ttreatment\n",
      "mid\ttreatment\n",
      "left\ttreatment\n",
      "anterior\ttreatment\n",
      "descending\ttreatment\n",
      "artery\ttreatment\n",
      ".\tO\n",
      "\n",
      "DISCHARGE\tO\n",
      "CONDITION\tO\n",
      ":\tO\n",
      "\n",
      "Good\tO\n",
      ".\tO\n",
      "\n",
      "DISCHARGE\tO\n",
      "MEDICATIONS\tO\n",
      ":\tO\n",
      "\n",
      "Aspirin\ttreatment\n",
      "325\tO\n",
      "mg\tO\n",
      "q.d.\tO\n",
      ",\tO\n",
      "Plavix\ttreatment\n",
      "75\tO\n",
      "mg\tO\n",
      "q.d.\tO\n",
      ",\tO\n",
      "Lipitor\ttreatment\n",
      "20\tO\n",
      "mg\tO\n",
      "q.d.\tO\n",
      ",\tO\n",
      "nitroglycerin\ttreatment\n",
      "0.3\tO\n",
      "mg\tO\n",
      "sublingually\tO\n",
      "p.r.n.\tO\n",
      "chest\tproblem\n",
      "pain\tproblem\n",
      "or\tO\n",
      "pressure\tproblem\n",
      "or\tO\n",
      "shortness\tproblem\n",
      "of\tproblem\n",
      "breath\tproblem\n",
      ",\tO\n",
      "lisinopril\ttreatment\n",
      "hydrochlorothiazide\ttreatment\n",
      "20\tO\n",
      "/\tO\n",
      "12.5\tO\n",
      "mg\tO\n",
      "q.d.\tO\n",
      ",\tO\n",
      "atenolol\ttreatment\n",
      "25\tO\n",
      "mg\tO\n",
      "q.d.\tO\n",
      "\n",
      "The\tperson\n",
      "patient\tperson\n",
      "was\tO\n",
      "told\tO\n",
      "he\tperson\n",
      "could\tO\n",
      "return\tO\n",
      "to\tO\n",
      "work\tO\n",
      "after\tO\n",
      "two\tO\n",
      "weeks\tO\n",
      "of\tO\n",
      "rest\ttreatment\n",
      ",\tO\n",
      "given\tO\n",
      "that\tpronoun\n",
      "his\tperson\n",
      "work\tO\n",
      "required\tO\n",
      "a\tO\n",
      "high\tO\n",
      "level\tO\n",
      "of\tO\n",
      "exertion\tO\n",
      "and\tO\n",
      "the\tperson\n",
      "patient\tperson\n",
      "was\tO\n",
      "also\tO\n",
      "given\tO\n",
      "a\tO\n",
      "prescription\tO\n",
      "for\tO\n",
      "a\ttreatment\n",
      "transdermal\ttreatment\n",
      "nicotine\ttreatment\n",
      "patch\ttreatment\n",
      "14\tO\n",
      "mg\tO\n",
      "the\tO\n",
      "be\tO\n",
      "used\tO\n",
      "for\tO\n",
      "up\tO\n",
      "to\tO\n",
      "2\tO\n",
      "weeks\tO\n",
      "until\tO\n",
      "cravings\tproblem\n",
      "remitted\tO\n",
      ".\tO\n",
      "\n",
      "Antonio\tperson\n",
      "PINTO\tperson\n",
      ",\tperson\n",
      "M.D.\tperson\n",
      "52-057\tO\n",
      "\n",
      "Dictated\tO\n",
      "By\tO\n",
      ":\tO\n",
      "Salvador\tperson\n",
      "BX\tperson\n",
      "Hernandez\tperson\n",
      ",\tperson\n",
      "M.D.\tperson\n",
      "\n",
      "MEDQUIST36\tO\n",
      "\n",
      "D\tO\n",
      ":\tO\n",
      "2014-10-18\tO\n",
      "12:39\tO\n",
      "\n",
      "T\tO\n",
      ":\tO\n",
      "2014-10-21\tO\n",
      "12:42\tO\n",
      "\n",
      "JOB\tO\n",
      "#:\tO\n",
      "78572\tO\n",
      "\n",
      "Signed\tO\n",
      "electronically\tO\n",
      "by\tO\n",
      ":\tO\n",
      "DR.\tperson\n",
      "Steve\tperson\n",
      "PINTO\tperson\n",
      "on\tO\n",
      ":\tO\n",
      "SUN\tO\n",
      "2014-11-02\tO\n",
      "8:39\tO\n",
      "PM\tO\n",
      "\n",
      "(\tO\n",
      "End\tO\n",
      "of\tO\n",
      "Report\tO\n",
      ")\tO\n"
     ]
    }
   ],
   "source": [
    "print (reprocessed_texts['clinical-273'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_txt = '\\n'.join(\n",
    "    [val for key,val in reprocessed_texts.items()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./NERdata.txt', mode='w') as f: f.write(merged_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_ner_train_files = glob('./NERdata/*/train.tsv')\n",
    "#all_ner_val_files = glob('./NERdata/*/devel.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read coNLL format data and store it in a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, filepath):\n",
    "        self.words, self.start_list, self.end_list  = self.read_conll_format(filepath)\n",
    "        self.labels = self.read_conll_format_labels(filepath)\n",
    "\n",
    "        assert len(self.words) == len(self.labels)\n",
    "\n",
    "        self.sentence = [\"sentence_{}\".format(i+1) for i in range(len(self.words))]\n",
    "        \n",
    "    def read_conll_format_labels(self, filename):\n",
    "        lines = self.read_lines(filename) + ['']\n",
    "        posts, post = [], []\n",
    "        for line in lines:\n",
    "            if line:\n",
    "                probs = line.split(\"\\t\")[1]\n",
    "                post.append(probs)\n",
    "                #print(\"post: \", post)\n",
    "            elif post:\n",
    "                posts.append(post)\n",
    "                post = []\n",
    "        # a list of lists of words/ labels\n",
    "        return posts\n",
    "\n",
    "    def read_conll_format(self, filename):\n",
    "        lines = self.read_lines(filename) + ['']\n",
    "        posts, post = [], []\n",
    "        start_list, end_list, starts, ends = [], [], [], []\n",
    "        start, end = 0, 0\n",
    "        for line in lines:\n",
    "            if line:\n",
    "                start = end + 1\n",
    "                words = line.split(\"\\t\")[0]\n",
    "                end = start + len(words) \n",
    "                # print(\"words: \", words)\n",
    "                post.append(words.lower())\n",
    "                starts.append(start)\n",
    "                ends.append(end)\n",
    "            elif post:\n",
    "                posts.append(post)\n",
    "                start_list.append(starts)\n",
    "                end_list.append(ends)\n",
    "                post = []\n",
    "                start, end = 0, 0\n",
    "        # a list of lists of words/ labels\n",
    "        return posts, start_list, end_list\n",
    "\n",
    "    def read_lines(self, filename):\n",
    "        with open(filename, 'r') as fp:\n",
    "            lines = [line.strip() for line in fp]\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset('./NERdata.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame()\n",
    "data_df['Sentence'] = data.sentence\n",
    "data_df['Word'] = data.words\n",
    "data_df['Entity'] = data.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>Entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence_1</td>\n",
       "      <td>[admission, date, :]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence_2</td>\n",
       "      <td>[2012-05-21]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence_3</td>\n",
       "      <td>[discharge, date, :]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence_4</td>\n",
       "      <td>[2012-05-25]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence_5</td>\n",
       "      <td>[date, of, birth, :]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sentence_6</td>\n",
       "      <td>[1957-01-05]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sentence_7</td>\n",
       "      <td>[sex, :]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sentence_8</td>\n",
       "      <td>[f]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sentence_9</td>\n",
       "      <td>[service, :]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sentence_10</td>\n",
       "      <td>[cmed, ccu]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence                  Word        Entity\n",
       "0   sentence_1  [admission, date, :]     [O, O, O]\n",
       "1   sentence_2          [2012-05-21]           [O]\n",
       "2   sentence_3  [discharge, date, :]     [O, O, O]\n",
       "3   sentence_4          [2012-05-25]           [O]\n",
       "4   sentence_5  [date, of, birth, :]  [O, O, O, O]\n",
       "5   sentence_6          [1957-01-05]           [O]\n",
       "6   sentence_7              [sex, :]        [O, O]\n",
       "7   sentence_8                   [f]           [O]\n",
       "8   sentence_9          [service, :]        [O, O]\n",
       "9  sentence_10           [cmed, ccu]        [O, O]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Sentence_length'] = data_df.Word.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14380.000000\n",
       "mean        10.095271\n",
       "std          9.111970\n",
       "min          1.000000\n",
       "25%          3.000000\n",
       "50%          7.000000\n",
       "75%         14.000000\n",
       "max         98.000000\n",
       "Name: Sentence_length, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.Sentence_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use texts which has atleast 10 words and at most 50 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asengup6\\softwares\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "min_length = 10\n",
    "max_length = 50\n",
    "\n",
    "data_df = data_df[data_df.Sentence_length >= min_length][data_df.Sentence_length <= max_length].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entity(text,labels):\n",
    "    original_text = \" \".join(text)\n",
    "    ents = []\n",
    "    start, end, start_tag = 0, 0, 0\n",
    "    for i, w in enumerate(text):\n",
    "        if i > 0:\n",
    "            start = end + 1\n",
    "            if labels[i-1] != labels[i]:\n",
    "                start_tag = start\n",
    "                \n",
    "        end = start + len(w)\n",
    "        \n",
    "        if labels[i] != 'O':\n",
    "            if i < len(text):\n",
    "                if labels[i+1] != labels[i]:\n",
    "                    ents.append({'start':start_tag,'end':end,'label':labels[i]})\n",
    "            else:\n",
    "                ents.append({'start':start_tag,'end':end,'label':labels[i]})\n",
    "    \n",
    "    sentence = [{'text': original_text, 'ents': ents,'title': None}]\n",
    "    \n",
    "    displacy.render(sentence,style='ent',manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    this\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">pronoun</span>\n",
       "</mark>\n",
       " is a 55-year-old female with multiple prior admissions for \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    pneumonia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">problem</span>\n",
       "</mark>\n",
       " , \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    copd\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">problem</span>\n",
       "</mark>\n",
       " , \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    asthma exacerbation\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">problem</span>\n",
       "</mark>\n",
       " , over 3 weeks of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    upper respiratory like infection\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">problem</span>\n",
       "</mark>\n",
       " unremitting with \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    increased nebulizer treatments\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">treatment</span>\n",
       "</mark>\n",
       " at home .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_entity(data_df.Word.iloc[0],data_df.Entity.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the patient\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">person</span>\n",
       "</mark>\n",
       " saw \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    her pcp\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">person</span>\n",
       "</mark>\n",
       " and was known to have \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    an oxygen saturation\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " of 82 to 85 percent on room air .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_entity(data_df.Word.iloc[1],data_df.Entity.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">in the emergency department , \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the patient\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">person</span>\n",
       "</mark>\n",
       " received \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    zithromax\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">treatment</span>\n",
       "</mark>\n",
       " , \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ceftriaxone\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">treatment</span>\n",
       "</mark>\n",
       " , and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    flagyl\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">treatment</span>\n",
       "</mark>\n",
       " .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_entity(data_df.Word.iloc[2],data_df.Entity.iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = GroupKFold(n_splits=5)\n",
    "\n",
    "for train_index, val_index in kf.split(data_df.Word,data_df.Entity,data_df.Sentence):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_df.iloc[train_index].reset_index(drop=True)\n",
    "val = data_df.iloc[val_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4520, 4) (1130, 4)\n"
     ]
    }
   ],
   "source": [
    "print (train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(elems):\n",
    "    return [e for elem in elems for e in elem]\n",
    "\n",
    "corpus = flatten(train.Word)\n",
    "elems, freqs = zip(*Counter(corpus).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 8598 words are in the train dataset\n"
     ]
    }
   ],
   "source": [
    "print (\"Total {} words are in the train dataset\".format(len(elems)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "embed_dim = 200 #dimension of the embedding\n",
    "lstm_out = 100 #dimension of the lstm output \n",
    "n_layers = 1 #number of LSTM layers\n",
    "MAX_NB_WORDS = 6000\n",
    "tags = list(label_vocab.values())\n",
    "n_tags = len(tags) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "We tokenize both the input and the output data. For input text data we use top 6000 words. Tokenization is done using keras tokenizer. Unknown words are replace with UNK tag. For shorter texts we use zero padding to make all the texts of same length.\n",
    "\n",
    "For output, we convert text entities into unique integer tokens and perform one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='UNK', num_words=MAX_NB_WORDS+1)\n",
    "tokenizer.fit_on_texts(train.Word)\n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= MAX_NB_WORDS+1}\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "\n",
    "# Vocabulary Key:token_index -> Value:word\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Vocabulary Key:Label/Tag -> Value:tag_index\n",
    "# The first entry is reserved for PAD\n",
    "tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "\n",
    "# Vocabulary Key:tag_index -> Value:Label/Tag\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "#tokenizer.word_index[tokenizer.oov_token] = MAX_NB_WORDS + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1,\n",
       " 'PAD': 0,\n",
       " 'person': 2,\n",
       " 'problem': 5,\n",
       " 'pronoun': 6,\n",
       " 'test': 4,\n",
       " 'treatment': 3}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4520, 50) (1130, 50) (4520, 50, 7) (1130, 50, 7)\n"
     ]
    }
   ],
   "source": [
    "trainX = pad_sequences(maxlen=max_length, sequences=tokenizer.texts_to_sequences(train.Word), padding=\"post\", value=word2idx[\"PAD\"])\n",
    "trainy = [[tag2idx[j] for j in i] for i in train.Entity]\n",
    "trainy = pad_sequences(maxlen=max_length, sequences=trainy, padding=\"post\", value=tag2idx[\"PAD\"])\n",
    "#trainy = [to_categorical(i, num_classes=n_tags) for i in tqdm(trainy)] \n",
    "\n",
    "valX = pad_sequences(maxlen=max_length, sequences=tokenizer.texts_to_sequences(val.Word), padding=\"post\", value=word2idx[\"PAD\"])\n",
    "valy = [[tag2idx[j] for j in i] for i in val.Entity]\n",
    "valy = pad_sequences(maxlen=max_length, sequences=valy, padding=\"post\", value=tag2idx[\"PAD\"])\n",
    "#valy = [to_categorical(i, num_classes=n_tags) for i in tqdm(valy)] \n",
    "\n",
    "trainX = np.array(trainX)\n",
    "trainy = np.array(trainy)\n",
    "valX = np.array(valX)\n",
    "valy = np.array(valy)\n",
    "\n",
    "trainy = to_categorical(trainy)\n",
    "valy = to_categorical(valy)\n",
    "\n",
    "print (trainX.shape, valX.shape, trainy.shape, valy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Embeddings\n",
    "\n",
    "We use pretrained PubMed word embeddings. In our NER model, the first layer is embedding layer. Instead of learning word embeddings in the NER model directly, we use pretrained embeddings which already contains rich semantic embedding of each word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2665548it [07:57, 5576.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4979 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('pubmed_wv.txt','r',encoding='utf8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    if coefs.shape[0] == embed_dim and word in word2idx:\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word2idx), embed_dim))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "We use 3 different models.\n",
    "\n",
    "* BiLSTM model - model uses 1-layer LSTM, intermediate dense layer and output softmax layer\n",
    "* BiLSTM-CNN model - model uses CNN layer for feature extraction, followed by BiLSTM, dense and output layer\n",
    "* BiLSTM-attention model - model contains BiLSTM layer, followed by multi headed self-attention layer and output layer\n",
    "\n",
    "We use categorical cross entropy to calculate loss between original entities and predicted entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \"\"\"Multi-headed attention layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, \n",
    "                 num_heads = 8, \n",
    "                 attention_dropout=.1,\n",
    "                 trainable=True,\n",
    "                 name='Attention'):\n",
    "        \n",
    "        if hidden_size % num_heads != 0:\n",
    "            raise ValueError(\"Hidden size must be evenly divisible by the number of heads.\")\n",
    "            \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.trainable = trainable\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dense = tf.keras.layers.Dense(self.hidden_size, use_bias=False)\n",
    "        super(Attention, self).__init__(name=name)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split x into different heads, and transpose the resulting value.\n",
    "        The tensor is transposed to insure the inner dimensions hold the correct\n",
    "        values during the matrix multiplication.\n",
    "        Args:\n",
    "          x: A tensor with shape [batch_size, length, hidden_size]\n",
    "        Returns:\n",
    "          A tensor with shape [batch_size, num_heads, length, hidden_size/num_heads]\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"split_heads\"):\n",
    "            batch_size = tf.shape(x)[0]\n",
    "            length = tf.shape(x)[1]\n",
    "\n",
    "            # Calculate depth of last dimension after it has been split.\n",
    "            depth = (self.hidden_size // self.num_heads)\n",
    "\n",
    "            # Split the last dimension\n",
    "            x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n",
    "\n",
    "            # Transpose the result\n",
    "            return tf.transpose(x, [0, 2, 1, 3])\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine tensor that has been split.\n",
    "        Args:\n",
    "          x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n",
    "        Returns:\n",
    "          A tensor with shape [batch_size, length, hidden_size]\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"combine_heads\"):\n",
    "            batch_size = tf.shape(x)[0]\n",
    "            length = tf.shape(x)[2]\n",
    "            x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n",
    "            return tf.reshape(x, [batch_size, length, self.hidden_size])        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Apply attention mechanism to inputs.\n",
    "        Args:\n",
    "          inputs: a tensor with shape [batch_size, length_x, hidden_size]\n",
    "        Returns:\n",
    "          Attention layer output with shape [batch_size, length_x, hidden_size]\n",
    "        \"\"\"\n",
    "        # Google developper use tf.layer.Dense to linearly project the queries, keys, and values.\n",
    "        q = self.dense(inputs)\n",
    "        k = self.dense(inputs)\n",
    "        v = self.dense(inputs)\n",
    "\n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        \n",
    "        # Scale q to prevent the dot product between q and k from growing too large.\n",
    "        depth = (self.hidden_size // self.num_heads)\n",
    "        q *= depth ** -0.5\n",
    "        \n",
    "        logits = tf.matmul(q, k, transpose_b=True)\n",
    "        # logits += self.bias\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
    "        \n",
    "        if self.trainable:\n",
    "            weights = tf.nn.dropout(weights, 1.0 - self.attention_dropout)\n",
    "        \n",
    "        attention_output = tf.matmul(weights, v)\n",
    "        attention_output = self.combine_heads(attention_output)\n",
    "        attention_output = self.dense(attention_output)\n",
    "        return attention_output\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm():\n",
    "    input = Input(shape=(max_length,))\n",
    "    x = Embedding(input_dim=len(word2idx), output_dim=embed_dim, weights=[embedding_matrix], trainable=False)(input)  # default: 100-dim embedding\n",
    "    x = SpatialDropout1D(.2)(x)\n",
    "    x = Bidirectional(LSTM(units=lstm_out, return_sequences=True))(x)  # biLSTM\n",
    "    x = SpatialDropout1D(.2)(x)\n",
    "    x = Dense(50)(x) # dense\n",
    "    x = SpatialDropout1D(.2)(x)\n",
    "    out = Dense(len(tag2idx),activation='softmax')(x) #output\n",
    "\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn_lstm():\n",
    "    input = Input(shape=(max_length,))\n",
    "    x = Embedding(input_dim=len(word2idx), output_dim=embed_dim, weights=[embedding_matrix], trainable=False)(input)  # default: 100-dim embedding\n",
    "    x = SpatialDropout1D(.2)(x)\n",
    "    x = Conv1D(filters=lstm_out,kernel_size=5,strides=1,padding='same')(x)\n",
    "    x = SpatialDropout1D(.2)(x)\n",
    "    x = Bidirectional(LSTM(units=lstm_out, return_sequences=True))(x)  # biLSTM\n",
    "    x = SpatialDropout1D(.2)(x)\n",
    "    x = Dense(50)(x) # dense\n",
    "    x = SpatialDropout1D(.2)(x)\n",
    "    out = Dense(len(tag2idx),activation='softmax')(x) #output\n",
    "\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm_attention():\n",
    "    input = Input(shape=(max_length,))\n",
    "    x = Embedding(input_dim=len(word2idx), output_dim=embed_dim, weights=[embedding_matrix], trainable=False)(input)  # default: 100-dim embedding\n",
    "    x = SpatialDropout1D(.2)(x)\n",
    "    x = Bidirectional(LSTM(units=lstm_out, return_sequences=True))(x)  # biLSTM\n",
    "    x = Attention(hidden_size=2*lstm_out)(x)\n",
    "    out = Dense(len(tag2idx),activation='softmax')(x) #output\n",
    "\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 50, 200)           1200400   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 50, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 50, 200)           240800    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 50, 200)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50, 50)            10050     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 50, 50)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50, 7)             357       \n",
      "=================================================================\n",
      "Total params: 1,451,607\n",
      "Trainable params: 251,207\n",
      "Non-trainable params: 1,200,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = model_lstm()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 200)           1200400   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 50, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 50, 100)           100100    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 200)           160800    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 50, 200)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50, 50)            10050     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial (None, 50, 50)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50, 7)             357       \n",
      "=================================================================\n",
      "Total params: 1,471,707\n",
      "Trainable params: 271,307\n",
      "Non-trainable params: 1,200,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = model_cnn_lstm()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 50, 200)           1200400   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_7 (Spatial (None, 50, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 50, 200)           240800    \n",
      "_________________________________________________________________\n",
      "Attention (Attention)        (None, None, 200)         40000     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, None, 7)           1407      \n",
      "=================================================================\n",
      "Total params: 1,482,607\n",
      "Trainable params: 282,207\n",
      "Non-trainable params: 1,200,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = model_lstm_attention()\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting and Evaluation\n",
    "\n",
    "We run each model on the training dataset for 25 epochs. As most of the word tokens are tagged with the tag \"O\", by default our problem is imbalance. In this case, accuracy metric is not suitable. We use a custom callback using macro F1 score. If the model achieves better F1 score, we store the model. Early stopping is performed using F1 score.\n",
    "\n",
    "During evaluation we use entity level classification report which calculates F1, precision, recall for each entity separately. The report describes how the model performs for different entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Callback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model, inputs, targets, filename, patience=5):\n",
    "        self.model = model\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets.argmax(-1).reshape(-1)\n",
    "        self.best_score = -1\n",
    "        self.bad_epoch = 0\n",
    "        self.filename = filename\n",
    "        self.patience = patience\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        pred = self.model.predict(self.inputs).argmax(-1).reshape(-1)\n",
    "        score = f1_score(self.targets, pred, average='macro')\n",
    "        print(f'\\nF1 Macro Score: {score:.5f}')\n",
    "        \n",
    "        if score > self.best_score:\n",
    "            self.best_score = score\n",
    "            self.bad_epoch = 0\n",
    "            self.model.save_weights(self.filename)\n",
    "            print (\"\\nModel saved in {}\".format(self.filename))\n",
    "        else:\n",
    "            self.bad_epoch += 1\n",
    "            \n",
    "        if self.bad_epoch >= self.patience:\n",
    "            print(\"\\nEpoch %05d: early stopping Threshold\" % epoch)\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4192 - accuracy: 0.8852\n",
      "F1 Macro Score: 0.65924\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 102ms/step - loss: 0.4178 - accuracy: 0.8853 - val_loss: 0.2144 - val_accuracy: 0.9291\n",
      "Epoch 2/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1981 - accuracy: 0.9349\n",
      "F1 Macro Score: 0.81503\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 98ms/step - loss: 0.1981 - accuracy: 0.9349 - val_loss: 0.1789 - val_accuracy: 0.9411\n",
      "Epoch 3/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9429\n",
      "F1 Macro Score: 0.83640\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 100ms/step - loss: 0.1726 - accuracy: 0.9429 - val_loss: 0.1569 - val_accuracy: 0.9463\n",
      "Epoch 4/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.1540 - accuracy: 0.9480\n",
      "F1 Macro Score: 0.84965\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 96ms/step - loss: 0.1539 - accuracy: 0.9480 - val_loss: 0.1462 - val_accuracy: 0.9497\n",
      "Epoch 5/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9515\n",
      "F1 Macro Score: 0.85769\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 13s 94ms/step - loss: 0.1417 - accuracy: 0.9515 - val_loss: 0.1385 - val_accuracy: 0.9523\n",
      "Epoch 6/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9553\n",
      "F1 Macro Score: 0.86706\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 99ms/step - loss: 0.1309 - accuracy: 0.9553 - val_loss: 0.1297 - val_accuracy: 0.9554\n",
      "Epoch 7/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9569\n",
      "F1 Macro Score: 0.86975\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 97ms/step - loss: 0.1249 - accuracy: 0.9569 - val_loss: 0.1257 - val_accuracy: 0.9564\n",
      "Epoch 8/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1189 - accuracy: 0.9596\n",
      "F1 Macro Score: 0.87818\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 96ms/step - loss: 0.1189 - accuracy: 0.9596 - val_loss: 0.1204 - val_accuracy: 0.9586\n",
      "Epoch 9/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9613\n",
      "F1 Macro Score: 0.87750\n",
      "142/142 [==============================] - 14s 98ms/step - loss: 0.1121 - accuracy: 0.9613 - val_loss: 0.1192 - val_accuracy: 0.9589\n",
      "Epoch 10/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9632\n",
      "F1 Macro Score: 0.88782\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 96ms/step - loss: 0.1054 - accuracy: 0.9632 - val_loss: 0.1137 - val_accuracy: 0.9615\n",
      "Epoch 11/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9647\n",
      "F1 Macro Score: 0.88550\n",
      "142/142 [==============================] - 14s 98ms/step - loss: 0.1021 - accuracy: 0.9647 - val_loss: 0.1142 - val_accuracy: 0.9606\n",
      "Epoch 12/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9657\n",
      "F1 Macro Score: 0.89021\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 96ms/step - loss: 0.0980 - accuracy: 0.9657 - val_loss: 0.1078 - val_accuracy: 0.9628\n",
      "Epoch 13/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9675\n",
      "F1 Macro Score: 0.89210\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 98ms/step - loss: 0.0928 - accuracy: 0.9675 - val_loss: 0.1090 - val_accuracy: 0.9632\n",
      "Epoch 14/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9685\n",
      "F1 Macro Score: 0.89290\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 97ms/step - loss: 0.0889 - accuracy: 0.9685 - val_loss: 0.1087 - val_accuracy: 0.9637\n",
      "Epoch 15/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9697\n",
      "F1 Macro Score: 0.89523\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 96ms/step - loss: 0.0868 - accuracy: 0.9697 - val_loss: 0.1057 - val_accuracy: 0.9640\n",
      "Epoch 16/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0816 - accuracy: 0.9714\n",
      "F1 Macro Score: 0.89691\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 16s 111ms/step - loss: 0.0814 - accuracy: 0.9714 - val_loss: 0.1090 - val_accuracy: 0.9639\n",
      "Epoch 17/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9725\n",
      "F1 Macro Score: 0.89954\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 15s 108ms/step - loss: 0.0784 - accuracy: 0.9725 - val_loss: 0.1086 - val_accuracy: 0.9639\n",
      "Epoch 18/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9727\n",
      "F1 Macro Score: 0.89814\n",
      "142/142 [==============================] - 15s 104ms/step - loss: 0.0767 - accuracy: 0.9727 - val_loss: 0.1056 - val_accuracy: 0.9653\n",
      "Epoch 19/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9740\n",
      "F1 Macro Score: 0.89812\n",
      "142/142 [==============================] - 14s 96ms/step - loss: 0.0731 - accuracy: 0.9740 - val_loss: 0.1049 - val_accuracy: 0.9651\n",
      "Epoch 20/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9751\n",
      "F1 Macro Score: 0.89898\n",
      "142/142 [==============================] - 14s 101ms/step - loss: 0.0715 - accuracy: 0.9750 - val_loss: 0.1053 - val_accuracy: 0.9651\n",
      "Epoch 21/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0695 - accuracy: 0.9755\n",
      "F1 Macro Score: 0.90186\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 98ms/step - loss: 0.0696 - accuracy: 0.9755 - val_loss: 0.1036 - val_accuracy: 0.9658\n",
      "Epoch 22/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9763\n",
      "F1 Macro Score: 0.89888\n",
      "142/142 [==============================] - 14s 101ms/step - loss: 0.0667 - accuracy: 0.9763 - val_loss: 0.1091 - val_accuracy: 0.9653\n",
      "Epoch 23/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9773\n",
      "F1 Macro Score: 0.90247\n",
      "\n",
      "Model saved in lstm.h5\n",
      "142/142 [==============================] - 14s 99ms/step - loss: 0.0647 - accuracy: 0.9773 - val_loss: 0.1024 - val_accuracy: 0.9660\n",
      "Epoch 24/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9779\n",
      "F1 Macro Score: 0.90212\n",
      "142/142 [==============================] - 15s 104ms/step - loss: 0.0616 - accuracy: 0.9779 - val_loss: 0.1056 - val_accuracy: 0.9661\n",
      "Epoch 25/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9790\n",
      "F1 Macro Score: 0.90075\n",
      "142/142 [==============================] - 17s 120ms/step - loss: 0.0601 - accuracy: 0.9790 - val_loss: 0.1087 - val_accuracy: 0.9662\n"
     ]
    }
   ],
   "source": [
    "f1callback1 = F1Callback(model1,valX,valy,'lstm.h5')\n",
    "\n",
    "history1 = model1.fit(trainX, trainy, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_data=(valX,valy),\n",
    "                   callbacks = [f1callback1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y_actual = valy.argmax(-1)\n",
    "val_y_actual = np.array([[idx2tag[i] for i in j] for j in val_y_actual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.93      0.95      0.94     14494\n",
      "         PAD       1.00      1.00      1.00     35337\n",
      "      person       0.94      0.87      0.90      1303\n",
      "     problem       0.84      0.81      0.82      2441\n",
      "     pronoun       0.96      0.97      0.97       209\n",
      "        test       0.89      0.83      0.86      1326\n",
      "   treatment       0.87      0.79      0.83      1390\n",
      "\n",
      "    accuracy                           0.97     56500\n",
      "   macro avg       0.92      0.89      0.90     56500\n",
      "weighted avg       0.97      0.97      0.97     56500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1.load_weights('lstm.h5')\n",
    "\n",
    "val_pred1 = model1.predict(valX).argmax(-1)\n",
    "val_pred1 = np.array([[idx2tag[i] for i in j] for j in val_pred1])\n",
    "\n",
    "report = flat_classification_report(y_pred=val_pred1, y_true=val_y_actual)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple BiLSTM model achieves 97% F1 score for entity pronoun, 90% for persons, 82% for problems, 86% for tests and 83% for treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3652 - accuracy: 0.8995\n",
      "F1 Macro Score: 0.67354\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 15s 107ms/step - loss: 0.3639 - accuracy: 0.8995 - val_loss: 0.1933 - val_accuracy: 0.9369\n",
      "Epoch 2/25\n",
      "  2/142 [..............................] - ETA: 5s - loss: 0.1756 - accuracy: 0.9444"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asengup6\\softwares\\anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/142 [============================>.] - ETA: 0s - loss: 0.1848 - accuracy: 0.9392\n",
      "F1 Macro Score: 0.81853\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 14s 96ms/step - loss: 0.1843 - accuracy: 0.9392 - val_loss: 0.1675 - val_accuracy: 0.9448\n",
      "Epoch 3/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1637 - accuracy: 0.9448\n",
      "F1 Macro Score: 0.84471\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 13s 93ms/step - loss: 0.1637 - accuracy: 0.9448 - val_loss: 0.1507 - val_accuracy: 0.9493\n",
      "Epoch 4/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.1514 - accuracy: 0.9488\n",
      "F1 Macro Score: 0.85065\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 13s 93ms/step - loss: 0.1515 - accuracy: 0.9488 - val_loss: 0.1462 - val_accuracy: 0.9503\n",
      "Epoch 5/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9517\n",
      "F1 Macro Score: 0.85545\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 13s 95ms/step - loss: 0.1404 - accuracy: 0.9517 - val_loss: 0.1381 - val_accuracy: 0.9528\n",
      "Epoch 6/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.1328 - accuracy: 0.9544\n",
      "F1 Macro Score: 0.86827\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 13s 94ms/step - loss: 0.1325 - accuracy: 0.9544 - val_loss: 0.1276 - val_accuracy: 0.9556\n",
      "Epoch 7/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9568\n",
      "F1 Macro Score: 0.86540\n",
      "142/142 [==============================] - 15s 103ms/step - loss: 0.1259 - accuracy: 0.9567 - val_loss: 0.1300 - val_accuracy: 0.9550\n",
      "Epoch 8/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9585\n",
      "F1 Macro Score: 0.86811\n",
      "142/142 [==============================] - 14s 99ms/step - loss: 0.1202 - accuracy: 0.9585 - val_loss: 0.1265 - val_accuracy: 0.9571\n",
      "Epoch 9/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.1145 - accuracy: 0.9608\n",
      "F1 Macro Score: 0.87897\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 14s 96ms/step - loss: 0.1147 - accuracy: 0.9608 - val_loss: 0.1200 - val_accuracy: 0.9591\n",
      "Epoch 10/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9619\n",
      "F1 Macro Score: 0.88060\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 14s 99ms/step - loss: 0.1100 - accuracy: 0.9619 - val_loss: 0.1202 - val_accuracy: 0.9593\n",
      "Epoch 11/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9634\n",
      "F1 Macro Score: 0.89141\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 14s 96ms/step - loss: 0.1058 - accuracy: 0.9634 - val_loss: 0.1111 - val_accuracy: 0.9620\n",
      "Epoch 12/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9651\n",
      "F1 Macro Score: 0.89232\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 14s 96ms/step - loss: 0.1018 - accuracy: 0.9651 - val_loss: 0.1109 - val_accuracy: 0.9623\n",
      "Epoch 13/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9660\n",
      "F1 Macro Score: 0.89051\n",
      "142/142 [==============================] - 13s 94ms/step - loss: 0.0986 - accuracy: 0.9660 - val_loss: 0.1104 - val_accuracy: 0.9624\n",
      "Epoch 14/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0953 - accuracy: 0.9669\n",
      "F1 Macro Score: 0.89563\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 14s 100ms/step - loss: 0.0951 - accuracy: 0.9669 - val_loss: 0.1116 - val_accuracy: 0.9623\n",
      "Epoch 15/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9680\n",
      "F1 Macro Score: 0.89258\n",
      "142/142 [==============================] - 13s 95ms/step - loss: 0.0919 - accuracy: 0.9680 - val_loss: 0.1104 - val_accuracy: 0.9632\n",
      "Epoch 16/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9691\n",
      "F1 Macro Score: 0.89092\n",
      "142/142 [==============================] - 14s 95ms/step - loss: 0.0887 - accuracy: 0.9691 - val_loss: 0.1117 - val_accuracy: 0.9624\n",
      "Epoch 17/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9702\n",
      "F1 Macro Score: 0.89397\n",
      "142/142 [==============================] - 14s 101ms/step - loss: 0.0862 - accuracy: 0.9702 - val_loss: 0.1073 - val_accuracy: 0.9638\n",
      "Epoch 18/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9712\n",
      "F1 Macro Score: 0.89668\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 16s 114ms/step - loss: 0.0818 - accuracy: 0.9712 - val_loss: 0.1062 - val_accuracy: 0.9643\n",
      "Epoch 19/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0797 - accuracy: 0.9722\n",
      "F1 Macro Score: 0.89732\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 14s 95ms/step - loss: 0.0799 - accuracy: 0.9721 - val_loss: 0.1083 - val_accuracy: 0.9645\n",
      "Epoch 20/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9727\n",
      "F1 Macro Score: 0.90021\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 13s 94ms/step - loss: 0.0785 - accuracy: 0.9727 - val_loss: 0.1053 - val_accuracy: 0.9649\n",
      "Epoch 21/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9737\n",
      "F1 Macro Score: 0.90101\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 14s 97ms/step - loss: 0.0750 - accuracy: 0.9737 - val_loss: 0.1072 - val_accuracy: 0.9653\n",
      "Epoch 22/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9751\n",
      "F1 Macro Score: 0.89869\n",
      "142/142 [==============================] - 13s 94ms/step - loss: 0.0718 - accuracy: 0.9750 - val_loss: 0.1100 - val_accuracy: 0.9645\n",
      "Epoch 23/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9748\n",
      "F1 Macro Score: 0.89866\n",
      "142/142 [==============================] - 14s 97ms/step - loss: 0.0720 - accuracy: 0.9747 - val_loss: 0.1063 - val_accuracy: 0.9655\n",
      "Epoch 24/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9750\n",
      "F1 Macro Score: 0.90218\n",
      "\n",
      "Model saved in cnn_lstm.h5\n",
      "142/142 [==============================] - 13s 94ms/step - loss: 0.0717 - accuracy: 0.9750 - val_loss: 0.1065 - val_accuracy: 0.9652\n",
      "Epoch 25/25\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.9759\n",
      "F1 Macro Score: 0.90027\n",
      "142/142 [==============================] - 13s 94ms/step - loss: 0.0679 - accuracy: 0.9759 - val_loss: 0.1087 - val_accuracy: 0.9653\n"
     ]
    }
   ],
   "source": [
    "f1callback2 = F1Callback(model2,valX,valy,'cnn_lstm.h5')\n",
    "\n",
    "history2 = model2.fit(trainX, trainy, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_data=(valX,valy),\n",
    "                   callbacks = [f1callback2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.94      0.94      0.94     14494\n",
      "         PAD       0.99      1.00      1.00     35337\n",
      "      person       0.93      0.87      0.90      1303\n",
      "     problem       0.84      0.80      0.82      2441\n",
      "     pronoun       0.97      0.97      0.97       209\n",
      "        test       0.86      0.87      0.87      1326\n",
      "   treatment       0.82      0.82      0.82      1390\n",
      "\n",
      "    accuracy                           0.97     56500\n",
      "   macro avg       0.91      0.90      0.90     56500\n",
      "weighted avg       0.96      0.97      0.96     56500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2.load_weights('cnn_lstm.h5')\n",
    "\n",
    "val_pred2 = model2.predict(valX).argmax(-1)\n",
    "val_pred2 = np.array([[idx2tag[i] for i in j] for j in val_pred2])\n",
    "\n",
    "report = flat_classification_report(y_pred=val_pred2, y_true=val_y_actual)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.6155 - accuracy: 0.8057\n",
      "F1 Macro Score: 0.30188\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 22s 154ms/step - loss: 0.6155 - accuracy: 0.8057 - val_loss: 0.3902 - val_accuracy: 0.8667\n",
      "Epoch 2/25\n",
      "  1/142 [..............................] - ETA: 0s - loss: 0.3914 - accuracy: 0.8644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asengup6\\softwares\\anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - ETA: 0s - loss: 0.3398 - accuracy: 0.8819\n",
      "F1 Macro Score: 0.49629\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 21s 147ms/step - loss: 0.3398 - accuracy: 0.8819 - val_loss: 0.3111 - val_accuracy: 0.8947\n",
      "Epoch 3/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.2806 - accuracy: 0.9043\n",
      "F1 Macro Score: 0.53163\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 22s 158ms/step - loss: 0.2806 - accuracy: 0.9043 - val_loss: 0.2829 - val_accuracy: 0.9070\n",
      "Epoch 4/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.9156\n",
      "F1 Macro Score: 0.56362\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 25s 176ms/step - loss: 0.2488 - accuracy: 0.9156 - val_loss: 0.2615 - val_accuracy: 0.9165\n",
      "Epoch 5/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.2270 - accuracy: 0.9236\n",
      "F1 Macro Score: 0.65382\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 25s 178ms/step - loss: 0.2270 - accuracy: 0.9236 - val_loss: 0.2346 - val_accuracy: 0.9253\n",
      "Epoch 6/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.2133 - accuracy: 0.9275\n",
      "F1 Macro Score: 0.69453\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 34s 238ms/step - loss: 0.2133 - accuracy: 0.9275 - val_loss: 0.2175 - val_accuracy: 0.9315\n",
      "Epoch 7/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1992 - accuracy: 0.9333\n",
      "F1 Macro Score: 0.71582\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 25s 177ms/step - loss: 0.1992 - accuracy: 0.9333 - val_loss: 0.2107 - val_accuracy: 0.9313\n",
      "Epoch 8/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1902 - accuracy: 0.9361\n",
      "F1 Macro Score: 0.75365\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 23s 160ms/step - loss: 0.1902 - accuracy: 0.9361 - val_loss: 0.2040 - val_accuracy: 0.9336\n",
      "Epoch 9/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1806 - accuracy: 0.9392\n",
      "F1 Macro Score: 0.77377\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 24s 168ms/step - loss: 0.1806 - accuracy: 0.9392 - val_loss: 0.1954 - val_accuracy: 0.9374\n",
      "Epoch 10/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9437\n",
      "F1 Macro Score: 0.79086\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 24s 171ms/step - loss: 0.1684 - accuracy: 0.9437 - val_loss: 0.1852 - val_accuracy: 0.9413\n",
      "Epoch 11/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1585 - accuracy: 0.9473\n",
      "F1 Macro Score: 0.81423\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 23s 161ms/step - loss: 0.1585 - accuracy: 0.9473 - val_loss: 0.1797 - val_accuracy: 0.9433\n",
      "Epoch 12/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1523 - accuracy: 0.9493\n",
      "F1 Macro Score: 0.80647\n",
      "142/142 [==============================] - 22s 158ms/step - loss: 0.1523 - accuracy: 0.9493 - val_loss: 0.1772 - val_accuracy: 0.9449\n",
      "Epoch 13/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1442 - accuracy: 0.9520\n",
      "F1 Macro Score: 0.82967\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 23s 159ms/step - loss: 0.1442 - accuracy: 0.9520 - val_loss: 0.1662 - val_accuracy: 0.9463\n",
      "Epoch 14/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.9459\n",
      "F1 Macro Score: 0.81639\n",
      "142/142 [==============================] - 21s 148ms/step - loss: 0.1733 - accuracy: 0.9459 - val_loss: 0.1731 - val_accuracy: 0.9459\n",
      "Epoch 15/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1418 - accuracy: 0.9533\n",
      "F1 Macro Score: 0.83342\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 21s 150ms/step - loss: 0.1418 - accuracy: 0.9533 - val_loss: 0.1720 - val_accuracy: 0.9485\n",
      "Epoch 16/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9555\n",
      "F1 Macro Score: 0.82977\n",
      "142/142 [==============================] - 21s 147ms/step - loss: 0.1354 - accuracy: 0.9555 - val_loss: 0.1626 - val_accuracy: 0.9502\n",
      "Epoch 17/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9576\n",
      "F1 Macro Score: 0.83594\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 21s 150ms/step - loss: 0.1285 - accuracy: 0.9576 - val_loss: 0.1627 - val_accuracy: 0.9515\n",
      "Epoch 18/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1226 - accuracy: 0.9601\n",
      "F1 Macro Score: 0.84641\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 21s 147ms/step - loss: 0.1226 - accuracy: 0.9601 - val_loss: 0.1596 - val_accuracy: 0.9526\n",
      "Epoch 19/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9616\n",
      "F1 Macro Score: 0.84526\n",
      "142/142 [==============================] - 21s 147ms/step - loss: 0.1183 - accuracy: 0.9616 - val_loss: 0.1623 - val_accuracy: 0.9510\n",
      "Epoch 20/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9617\n",
      "F1 Macro Score: 0.84815\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 21s 145ms/step - loss: 0.1154 - accuracy: 0.9617 - val_loss: 0.1608 - val_accuracy: 0.9518\n",
      "Epoch 21/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1124 - accuracy: 0.9628\n",
      "F1 Macro Score: 0.85152\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 21s 148ms/step - loss: 0.1124 - accuracy: 0.9628 - val_loss: 0.1616 - val_accuracy: 0.9532\n",
      "Epoch 22/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9640\n",
      "F1 Macro Score: 0.86466\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 21s 149ms/step - loss: 0.1092 - accuracy: 0.9640 - val_loss: 0.1470 - val_accuracy: 0.9553\n",
      "Epoch 23/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9647\n",
      "F1 Macro Score: 0.86073\n",
      "142/142 [==============================] - 21s 146ms/step - loss: 0.1065 - accuracy: 0.9647 - val_loss: 0.1517 - val_accuracy: 0.9547\n",
      "Epoch 24/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9658\n",
      "F1 Macro Score: 0.86034\n",
      "142/142 [==============================] - 21s 147ms/step - loss: 0.1029 - accuracy: 0.9658 - val_loss: 0.1551 - val_accuracy: 0.9554\n",
      "Epoch 25/25\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9665\n",
      "F1 Macro Score: 0.86694\n",
      "\n",
      "Model saved in lstm_attention.h5\n",
      "142/142 [==============================] - 21s 148ms/step - loss: 0.1001 - accuracy: 0.9665 - val_loss: 0.1481 - val_accuracy: 0.9565\n"
     ]
    }
   ],
   "source": [
    "f1callback3 = F1Callback(model3,valX,valy,'lstm_attention.h5')\n",
    "\n",
    "history3 = model3.fit(trainX, trainy, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_data=(valX,valy),\n",
    "                   callbacks = [f1callback3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.92      0.94      0.93     14494\n",
      "         PAD       1.00      0.99      0.99     35337\n",
      "      person       0.91      0.85      0.88      1303\n",
      "     problem       0.80      0.76      0.78      2441\n",
      "     pronoun       0.95      0.89      0.92       209\n",
      "        test       0.85      0.78      0.81      1326\n",
      "   treatment       0.79      0.75      0.77      1390\n",
      "\n",
      "    accuracy                           0.96     56500\n",
      "   macro avg       0.89      0.85      0.87     56500\n",
      "weighted avg       0.96      0.96      0.96     56500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3.load_weights('lstm_attention.h5')\n",
    "\n",
    "val_pred3 = model3.predict(valX).argmax(-1)\n",
    "val_pred3 = np.array([[idx2tag[i] for i in j] for j in val_pred3])\n",
    "\n",
    "report = flat_classification_report(y_pred=val_pred3, y_true=val_y_actual)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Validation\n",
    "\n",
    "We convert the fixed length outputs to original lengths and inspect predictions on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1130/1130 [00:01<00:00, 938.72it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted_entity = []\n",
    "\n",
    "for i,s in enumerate(tqdm(val.Sentence.unique())):\n",
    "    len_sentence = len(val[val.Sentence == s].Word.iloc[0])\n",
    "    if len_sentence < max_length:\n",
    "        predicted_entity.append(val_pred1[i,:len_sentence].tolist())\n",
    "    else:\n",
    "        predicted_entity.append(val_pred1[i][:max_length].tolist() + ['O']*(len_sentence - max_length))\n",
    "        \n",
    "val['Predicted_Entity'] = predicted_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentence_length</th>\n",
       "      <th>Predicted_Entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence_13</td>\n",
       "      <td>[the, patient, saw, her, pcp, and, was, known,...</td>\n",
       "      <td>[person, person, O, person, person, O, O, O, O...</td>\n",
       "      <td>22</td>\n",
       "      <td>[person, person, O, person, person, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence_16</td>\n",
       "      <td>[chest, x-ray, was, remarkable, for, a, questi...</td>\n",
       "      <td>[test, test, O, O, O, O, O, problem, problem, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>[test, test, O, O, O, problem, problem, proble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence_19</td>\n",
       "      <td>[asthma, /, copd, on, bipap, ,, history, of, i...</td>\n",
       "      <td>[problem, O, problem, O, treatment, O, O, O, t...</td>\n",
       "      <td>10</td>\n",
       "      <td>[problem, O, problem, O, treatment, O, O, O, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence_43</td>\n",
       "      <td>[from, admission, ,, vital, signs, 98.3, ,, bl...</td>\n",
       "      <td>[O, O, O, test, test, O, O, test, test, O, O, ...</td>\n",
       "      <td>29</td>\n",
       "      <td>[O, O, O, test, test, O, O, test, test, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence_45</td>\n",
       "      <td>[she, is, a, chronically, ill-appearing, femal...</td>\n",
       "      <td>[person, O, O, O, O, O, O, O, O, problem, prob...</td>\n",
       "      <td>12</td>\n",
       "      <td>[person, O, O, O, O, O, O, O, O, problem, prob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence                                               Word  \\\n",
       "0  sentence_13  [the, patient, saw, her, pcp, and, was, known,...   \n",
       "1  sentence_16  [chest, x-ray, was, remarkable, for, a, questi...   \n",
       "2  sentence_19  [asthma, /, copd, on, bipap, ,, history, of, i...   \n",
       "3  sentence_43  [from, admission, ,, vital, signs, 98.3, ,, bl...   \n",
       "4  sentence_45  [she, is, a, chronically, ill-appearing, femal...   \n",
       "\n",
       "                                              Entity  Sentence_length  \\\n",
       "0  [person, person, O, person, person, O, O, O, O...               22   \n",
       "1  [test, test, O, O, O, O, O, problem, problem, ...               12   \n",
       "2  [problem, O, problem, O, treatment, O, O, O, t...               10   \n",
       "3  [O, O, O, test, test, O, O, test, test, O, O, ...               29   \n",
       "4  [person, O, O, O, O, O, O, O, O, problem, prob...               12   \n",
       "\n",
       "                                    Predicted_Entity  \n",
       "0  [person, person, O, person, person, O, O, O, O...  \n",
       "1  [test, test, O, O, O, problem, problem, proble...  \n",
       "2  [problem, O, problem, O, treatment, O, O, O, t...  \n",
       "3  [O, O, O, test, test, O, O, test, test, O, O, ...  \n",
       "4  [person, O, O, O, O, O, O, O, O, problem, prob...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the patient\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">person</span>\n",
       "</mark>\n",
       " saw \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    her pcp\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">person</span>\n",
       "</mark>\n",
       " and was known to have \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    an oxygen saturation\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " of 82 to 85 percent on room air .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the patient\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">person</span>\n",
       "</mark>\n",
       " saw \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    her pcp\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">person</span>\n",
       "</mark>\n",
       " and was known to have \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    an oxygen saturation\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " of 82 to 85 percent on room air .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Actual :\")\n",
    "plot_entity(val.Word.iloc[0],val.Entity.iloc[0])\n",
    "print (\"Predicted :\")\n",
    "plot_entity(val.Word.iloc[0],val.Predicted_Entity.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    chest x-ray\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " was remarkable for a questionable \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    left lower lobe infiltrate\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">problem</span>\n",
       "</mark>\n",
       " .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    chest x-ray\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " was remarkable for \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    a questionable left lower lobe infiltrate\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">problem</span>\n",
       "</mark>\n",
       " .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Actual :\")\n",
    "plot_entity(val.Word.iloc[1],val.Entity.iloc[1])\n",
    "print (\"Predicted :\")\n",
    "plot_entity(val.Word.iloc[1],val.Predicted_Entity.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    asthma\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">problem</span>\n",
       "</mark>\n",
       " / \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    copd\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">problem</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    bipap\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">treatment</span>\n",
       "</mark>\n",
       " , history of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    intubation\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">treatment</span>\n",
       "</mark>\n",
       " .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    asthma\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">problem</span>\n",
       "</mark>\n",
       " / \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    copd\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">problem</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    bipap\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">treatment</span>\n",
       "</mark>\n",
       " , history of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    intubation\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">treatment</span>\n",
       "</mark>\n",
       " .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Actual :\")\n",
    "plot_entity(val.Word.iloc[2],val.Entity.iloc[2])\n",
    "print (\"Predicted :\")\n",
    "plot_entity(val.Word.iloc[2],val.Predicted_Entity.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">from admission , \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    vital signs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " 98.3 , \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    blood pressure\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " 128/74 , 68 to 112 is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    her heart rate\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " , \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    respiratory rate\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " 24 , 100 percent on room air .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">from admission , \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    vital signs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " 98.3 , \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    blood pressure\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " 128/74 , 68 to 112 is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    her heart rate\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " , \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    respiratory rate\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">test</span>\n",
       "</mark>\n",
       " 24 , 100 percent on room air .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Actual :\")\n",
    "plot_entity(val.Word.iloc[3],val.Entity.iloc[3])\n",
    "print (\"Predicted :\")\n",
    "plot_entity(val.Word.iloc[3],val.Predicted_Entity.iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    she\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">person</span>\n",
       "</mark>\n",
       " is a chronically ill-appearing female , in no \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    acute distress\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">problem</span>\n",
       "</mark>\n",
       " .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    she\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">person</span>\n",
       "</mark>\n",
       " is a chronically ill-appearing female , in no \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    acute distress\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">problem</span>\n",
       "</mark>\n",
       " .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Actual :\")\n",
    "plot_entity(val.Word.iloc[4],val.Entity.iloc[4])\n",
    "print (\"Predicted :\")\n",
    "plot_entity(val.Word.iloc[4],val.Predicted_Entity.iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
